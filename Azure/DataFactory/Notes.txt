--Create Data Factory   
    --Create resource group: covid-reporting-rg 
    --covid-reporting-adf
    --pin create new: Covid Reporting project 
        --verify Dashboard

--Create Storage Account
    --covidreportingsa
    --performance - standard
    --Redundancy - locally 
    --Review and Create
    --go to resource 
    --pin service

    --Access Keys

--Storage Explorer
    -- covidvkpreportingsa  -> Storage Browser -> Download Azure Storage Explorer 

--Azure Data Lake Gen2 account 
    Azure Date Lake Storage Gen2 is built on top of the standard storage account, but it provides additional features such as hierarchical namespace, fine grain security, and compatibility with big data workloads.
    So it's more suited to be used as a Data Lake for big data analytics.
    --Storage account
    --Name: covidreportingdl
    --Advanced: Enable hierarchical namespace
    --Review + create
    --In the account: Properties
    --pin service

--sql database
    db name: covid-db
    server: create new     
            Server: covid-srv 
            Authentication method: User SQL Authentication
                        Server admin: admin 

            Workload: development
            Compute+storage: configure database 
                        Service tier: basic
            Backup storage Redundancy: local

    Networking: 
        Connectivity method: Public endpoint
        Allow Azure services to access: Yes
        Add current ip address: Yes (desktop can interact with the db)

    Review, create

    Go to service account: 
        Server name, Pricing tier, storage(2gb)

---------------------------------------------------------------------
--Section 4: Data Ingestion from Azure blob
---------------------------------------------------------------------
Copy activity from Azure blob storage to Azure Data Lake

population_by_age.tsv: country_codes, year age range
    filter by UK
    : empty data, p: provisional, e: estimated 

upload this file into blob storage
gz to raw (copy activity)

in order to extract the data. We need connection detail to the blob storage and to the Data Lake.
So in Data factory, these connection details are represented by link services.
And finally, the copy activity itself is not executable. So we need a pipeline to wrap all of this up so that it can be executed.

There are three broad areas here:
the source, the pipeline, and the sink.

1. In covidreportingsa: create blob container - population
    upload: population_by_age.tsv.gz

2. create "raw" container in datalake: covidreportingdl

3. Naming conventions: ls: linked services, ds: dataset, pl: pipeline
        ablob: azure blob storage, adls: azure datalake storage

4. Complete steps in the order mentioned in the slide (61). 

5. Create Linked Services: 
    Open ADF service -> manage -> Linked services -> New
    -> Azure Blob Storage ->  Name: ls_ablob_covidreportingsa
    -> Azure subscription -> 
        Storage account: covidreportingsa 
    -> Test and create

    -> New -> ADLS Gen2 -> ls_adls_covidreportingdl 
    -> Azure subscription -> Storage account: covidreportingdl -> Test and create

6. Create Datasets: 
    -> Author -> Datasets (new) -> Azure Blob Storage -> Delimited Text 
    ->  Name: ds_population_raw_gz 
        Linked Service: ls_ablob_covidreportingsa
        File path: population/population_by_age.tsv.gz 
        Import schema: None (we're not examining the data, just copying)
    ->  compression type: gzip
        compression level: optimal
        delimiter: tab
        preview data

    Datasets (new) -> ADLS Gen2 -> Delimited Text 
    ->  Name: ds_population_raw_tsv
        Linked Service: ls_adls_covidreportingdl 
        File path: raw/population/population_by_age.tsv
        Import schema: None
    ->  compression type: none
        delimiter: tab
    ->  validate all
    ->  publish all

7. Create pipeline
    -> New pipeline: pl_ingest_population_data
    -> Activities: 
        General: 
        drag 'Copy data', Name: Copy Population Data
        Timeout: 10mins

        Source: ds_population_raw_gz

        Sink: ds_population_raw_tsv

        validate
        Debug (input, output, details)
        Publish all

        Verify in Azure Storage Explorer

8. Validation activity
    Scenario:1
    -> Drag Validation activity, name: Check if file exists
    -> Settings: 
        Dataset: ds_population_raw_gz
        Timeout: 1:00:00:00 (testing: 0:00:00:30 seconds)
        Sleep: 600 seconds (Testing: 10 seconds)
        Minimum size: 1024
    -> Drag Success button, connect to Copy data
    -> Delete source file (.gz) from Storage Explorer
    -> Test1: wait for validation to fail (file doesn't exist)
    -> Test2: put file in Storage Explorer
            Timeout: 1:00:00:00 (testing: 0:00:02:00 minutes)

    Scenario:2
    -> Drag Get Metadata, Name: Get File Metadata
    -> Dataset: ds_population_raw_gz
        Field list: Column count, Size, Exists
        (Debug to get details from output for above values)

    -> Drag If condition
        Connect from Get Metadata Success to If Condition
        Activities -> Expression -> Add dynamic content
            -> Logical Functions -> equals
            -> @equals(activity('Get File Metadata').output.columnCount, 13)
            -> Move Copy Population Data for True
            -> Tie Validation activity to Get Metadata activity
            -> Debug
            
            -> Change expected column count to 14 to fail
                (Succeeds but doesn't perform anything)
            -> False -> Drag Web activity -> 
                Name: Send Email
                Settings: 
                    URL:https://file_incompatible
                    Method: POST 
                    Body: Dummy 

    Scenario: 3   (Lesson 27)
        (Delete source file on successful copy)
        -> If Condition -> True -> Drag Delete activity (Delete source file)
        -> Source: Dataset: ds_population_raw_gz
        -> Enable logging: uncheck
        -> Link Copy activity to Delete activity
        Debug

9. Triggers: (slide 67)
        Mange -> Triggers -> New -> 
        -> Name: tr_ingest_population_data
        -> Storage Event trigger
        -> select subscription, storage account name, container name
        -> Blob path begins with: population_by_age.tsv.gz
        -> Event: Blob created: check
        -> Start trigger on creation: check
        -> Create
        -> Add trigger in pl_ingest_population_data pipeline
        -> Publish

        <Register Microsoft.EventGrid from subscription -> Resource providers>

---------------------------------------------------------------------
--Section 4: Data Ingestion from HTTP (slide 74)
---------------------------------------------------------------------
ecdc.europa.eu/en/covid-19/data

https://www.ecdc.europa.eu/en/publications-data/data-national-14-day-notification-rate-covid-19

Github: https://github.com/cloudboxacademy/covid19

BaseUrl - https://raw.githubusercontent.com
RelativeUrl for each of the file will be
cloudboxacademy/covid19/main/ecdc_data/cases_deaths.csv
cloudboxacademy/covid19/main/ecdc_data/hospital_admissions.csv
cloudboxacademy/covid19/main/ecdc_data/testing.csv
cloudboxacademy/covid19/main/ecdc_data/country_response.csv

HTTP ---> Azure Data Lake

1. Create pipeline (Slide 79)
    Linked Serivces: HTTP 
    Name: ls_http_opendata_ecdc_europa_eu
    Base URL: https://raw.githubusercontent.com
    Authentication type: Anonymous
    Test connection, create

2. Datasets: 
    Source: New -> HTTP -> Delimited text
    Name: ds_cases_deaths_raw_csv_http
    Linked Services: ls_http_opendata_ecdc_europa_eu
    Relative URL: cloudboxacademy/covid19/main/ecdc_data/cases_deaths.csv
    First row header

    Sink: New -> ADLS Gen2 -> Delimited text
    Name: ds_cases_deaths_raw_csv_dl
    Linked Services: ls_adls_covidreportingdl
    File path: raw/ecdc/cases_deaths.csv
    First row header

3. Create pipeline: 
    Name: pl_ingest_cases_deaths_data
    Drag: Copy activity -> Name: Copy Cases and Deaths Data
    Source: ds_cases_deaths_raw_csv_http
    Sink: ds_cases_deaths_raw_csv_dl

    Debug
    Check in Storage Explorer
    Publish all

Variables and Parameters
-------------------------
Parameters are external values passed into pipelines, datasets or linked services. The value cannot be changed inside a pipeline.
Variables are internal values set inside a pipeline. The value can be changed inside the pipeline using Set Variable or Append Variable Activity

Source is same but relative urls are different. 
Sink destination (linked services) is same but file names are different

1. Create parameters for source and sink data sets. 
    goto source ds: ds_cases_deaths_raw_csv_http
        Parameters: New
        Name: relativeURL

        Connection: Relative URL -> Add dynamic content
            -> Parameters: relativeURL (@dataset().relativeURL)
            Finish

    goto sink ds: ds_cases_deaths_raw_csv_dl
        (parameterize the file name)
        Parameters: New
        Name: fileName

        Connection: File Path -> File: Add dynamic content
            -> Parameters: fileName (@dataset().fileName)
            Finish

2.  Create pipeline with variables
    goto Pipeline: pl_ingest_cases_deaths_data
        Variables: New
        Name: sourceRelativeURL
        Default value: cloudboxacademy/covid19/main/ecdc_data/hospital_admissions.csv

        Name: sinkFileName
        Default value: hospital_admissions.csv

        Click on Copy activity: 
            Source: relativeURL: Add dynamic content 
            Select variable: sourceRelativeURL

            Sink: fileName: Add dynamic content 
            Select variable: sinkFileName

        Debug

3. parameterize the pipeline itself so that it becomes truly generic, and then we can pass on the parameters from the trigger. 
   Convert variables into parameters

    goto Pipeline: pl_ingest_cases_deaths_data
    Parameters -> New
        sourceRelativeURL
        sinkFileName
    No default values. We'll pass them during run time. 

    Delete 2 variables

    goto Copy activity: 
        Sink -> replace parameter: @pipeline().parameters.sinkFileName
        Source -> replace parameter: @pipeline().parameters.sourceRelativeURL

    Rename pipeline and data set names to be generic: 
        ds_cases_deaths_raw_csv_http -> ds_ecdc_raw_csv_http
        ds_cases_deaths_raw_csv_dl -> ds_ecdc_raw_csv_dl
        pl_ingest_cases_deaths_data -> pl_ingest_ecdc_data
        Copy activity: 
        Copy Cases and Deaths Data -> Copy ECDC Data

    verify Source and Sink of copy activity is pointing to the new data set names. 

    Debug

    Pass parameter values: 
    sourceRelativeURL: cloudboxacademy/covid19/main/ecdc_data/hospital_admissions.csv
    sinkFileName: hospital_admissions.csv

    Monitor pipeline

    Triggers: pass in parameter values during run time. 

    Manage -> Triggers -> New
    (Since the data will always be there, we'll be looking at getting the data at a certain point, we'll use scheduled trigger)

    -> Name: tr_ingest_hospital_admissions_data
    -> Type: schedule
    -> Recurrence: 24hrs
    -> Activated: Yes
    -> Before publishing, attach trigger to pipeline: pl_ingest_ecdc_data
    -> Ok
    -> Pass parameters: 
        sourceRelativeURL: cloudboxacademy/covid19/main/ecdc_data/hospital_admissions.csv
        sinkFileName: hospital_admissions.csv
    -> Publish
    (In trigger, ensure start data is 2 mins later)

4. To run 4 files, we need to create 4 triggers. To make it further generic, we'll use one trigger to accept any number of files. 
Data Factory provides activities called Lookup and ForEach to get the information from a data source which is supported by Azure.

    -> Create JSON file: ecdc_file_list_for_2_files.json
    upload to new folder: covidreportingsa/config
    [
        {
            "sourceRelativeURL":"cloudboxacademy/covid19/main/ecdc_data/cases_deaths.csv",
            "sinkFileName":"cases_deaths.csv"
        },
        {
            "sourceRelativeURL":"cloudboxacademy/covid19/main/ecdc_data/hospital_admissions.csv",
            "sinkFileName":"hospital_admissions.csv"
        }
    ]
    -> in ADF, create new data set: Azure Blob Storage -> JSON 
        -> Name: ds_ecdc_file_list
        -> Linked Serivces: ls_ablob_covidreportingsa
        -> File Path: config/ecdc_file_list_for_2_files.json
        -> Import schema
        -> OK
    >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
    To understand how Lookup activity works: 
        Create test pipeline
        Drag Lookup activity
        Settings: Source dataset -> ds_ecdc_file_list
        First row only: uncheck
        Debug -> Check output

        Drag ForEach activity
        Tie Lookup to ForEach
        Settings: 
            Sequential
            Items: Add dynamic content
            @activity('Lookup1').output.value
            Finish

        Test pipeline: 
            variables: New
            sourceRelativeURL

            select ForEach -> Activities -> edit ForEach
            -> Drag Set Variable activity
            -> Variables: 
                Name: sourceRelativeURL
                Value: Add dynamic content
                    @item().sourceRelativeURL

            Debug -> check output 
            (Lookup1, ForEach1, Set Variable1, Set Variable1)

            Discard changes for test pipeline
    >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

    Publish

5. Linked Service Parameters: 
    Pass sourceURL as parameter (incase if different for other files)

    Linked Services -> ls_http_opendata_ecdc_europa_eu -> Parameters -> New -> Name: sourceBaseURL
    -> Base URL: clear text field -> Add dynamic content 
        -> sourceBaseURL (@linkedService().sourceBaseURL)
        -> Apply

    -> Author tab 
        -> find dataset which uses above linked service 
            ds_ecdc_raw_csv_http
        -> Parameters: New -> baseURL
        -> Connections: sourceBaseURL -> Add dynamic content
            -> @dataset().baseURL

    -> Pipeline: pl_ingest_ecdc_data
        Parameters: Add new: sourceBaseURL

    -> Copy activity: Source
        -> baseURL: Add dynamic content
            -> @pipeline().parameters.sourceBaseURL
        -> Finish
    Debug

    sourceRelativeURL:cloudboxacademy/covid19/main/ecdc_data/country_response.csv
    sinkFileName: country_response.csv
    sourceBaseURL: https://raw.githubusercontent.com

    Publsh All

6. Metadata driver pipeline
    Bring everything we learn together to process all 4 files. 

    create file in covidreportingsa/configs: ecdc_file_list.json
    [
    {
        "sourceBaseURL":"https://raw.githubusercontent.com",
        "sourceRelativeURL":"cloudboxacademy/covid19/main/ecdc_data/cases_deaths.csv",
        "sinkFileName":"cases_deaths.csv"
    },
    {
        "sourceBaseURL":"https://raw.githubusercontent.com",
        "sourceRelativeURL":"cloudboxacademy/covid19/main/ecdc_data/hospital_admissions.csv",
        "sinkFileName":"hospital_admissions.csv"
    },
    {
        "sourceBaseURL":"https://raw.githubusercontent.com",
        "sourceRelativeURL":"cloudboxacademy/covid19/main/ecdc_data/testing.csv",
        "sinkFileName":"testing.csv"
    },
    {
        "sourceBaseURL":"https://raw.githubusercontent.com",
        "sourceRelativeURL":"cloudboxacademy/covid19/main/ecdc_data/country_response.csv",
        "sinkFileName":"country_response.csv"
    }
    ]

    -> In pipeline: pl_ingest_ecdc_data
    -> Drag lookup activity -> Name: Lookup ECDC File list
    -> Settings -> Dataset: ds_ecdc_file_list
            First row only: uncheck
    
    -> Drag ForEach activity, Name: Execute Copy For Every Record
    -> Settings: Items: @activity('Lookup ECDC File list').output.value 
            Sequential: uncheck
    -> Move Copy activity inside ForEach
        
    -> Pipeline -> Delete parameters (they'll come from Lookup)

    -> ForEach activity -> Copy ECDC Data 
        -> Source
            -> relativeURL: @item().sourceRelativeURL
            -> baseURL: @item().sourceBaseURL

        -> Sink 
            -> fileName: @item().sinkFileName

    Correct the source file in ds_ecdc_file_list:
        Connection -> File path -> ecdc_file_list.json 
    Validate -> Debug
    Storage Explorer should have 4 files 

    Create trigger: 
    Delete: tr_ingest_hospital_admissions_data

    New: tr_ingest_ecdc_data
    Type: schedule
    Start date: 3mins ahead
    Activate
    Attach trigger to pipeline: pl_ingest_ecdc_data
    Publish

---------------------------------------------------------------------
--Section 6: Data Flows  
---------------------------------------------------------------------

The data flows themselves are not executable, so you'll have to build a pipeline to execute a data flow.
Should have atleast one source transformation and one sink transformation. 

Transform cases_deaths.csv
slide-93 to verify the transformation
upload country_lookup.csv to covidreportingdl/lookup
Test file: 
    create a smaller file for our input so that we can easily debug.
        In cases_deaths.csv filter one Europe country and one non-Europe
        Filter by GBR, IND and save to new file: cases_deaths_uk_ind_only.csv
        Upload to SA: covidreportingdl/raw/ecdc
        This is a test file and will delete once we verify our tests. 

1. Source Transformation
    -> Turn on the debug: select default integration runtime
        This will create a cluster with 4 CPUs. If needed a bigger one, manage->integration runtimes
    -> Data Flow -> New -> Name: df_transform_cases_deaths
    -> Add source: 
        Source settings: 
            (Data is flowing through the data flow are called stream.)
            Output stream name: CasesAndDeathsSource
            Source type: Dataset
            Dataset: New - ADLS Gen2 - Delimitted text 
                Name: ds_raw_cases_and_deaths
                Linked Services: ls_adls_covidreportingdl
                File path: raw/ecdc/cases_deaths.csv
                First row header
                Import schema: From connection/store
                Ok
            -> Options: Allow schema drift 
                    (For Validate schema, check dataset schema. The incoming file should match exactly as this schema.)
            -> Sampling: Disabled (We'll use our test file to explicitly test the data we know, rather than ADF picking some sample data)
            -> Debug settings 
                Source dataset: Row limit: 2000
                Sample file: raw/ecdc/cases_deaths_uk_ind_only.csv

        Source options: (similar to copy activity)
        Projection: Detect data type
        Optimize: (covered later for performance optimzation)
        Data preview: (refresh)

2. Filter Transformation
    (+) on Source transformation -> Filter
        -> Output stream name: FilterEuropeOnly
        -> Incoming stream: CasesAndDeathsSource
        -> Filter on: continent == 'Europe' && not (isNull(country_code))
            (include continent: Europe, with country_code - exclue blanks)
            Data preview
            Save and finish
        -> Data preview
        -> Inspect tab

3. Select Transformation
    (+) on Filter transformation -> Select
    -> Output stream name: SelectOnlyRequiredFields
    -> Incoming stream: FilterEuropeOnly
    -> Add mapping: 
            >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
            Fixed mapping: add column and inspect 
                    Skip duplicate input columns
            Rule based mapping: 
                Delete all columns 
                Enter matching conditions: open expression builder                    
                    Column names prefixed with 'cases_': 
                        matching condition: true()
                        Output column name expression: 'cases_' + $$
                        Save and Finish
                        Inspect 
                    String columns append with '_string'
                        matching condition: type == 'string'
                        Output column name expression: $$ + '_string'
                        Save and Finish
                        Inspect 
                    Reset
            >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

            Delete Continent, rate_14_day, date (add rule based mapping)
            Add mapping -> Rule based mapping
                        matching condition: name == 'date'
                        Output column name expression: 'reported' + '_date'
                        Save and Finish
                        Inspect

4. Pivot Transformation
    Indicator field has Confirmed cases and death. We'll pivot data to get count for each indicator.
    Pivot key: indicator, column being pivoted: daily_count. Other columns used in group by

    (+) on Select transformation -> Pivot
    Name: PivotCounts
    Group by: All columns except pivot key and column being pivoted. 
    Pivot key: indicator 
        Value:  confirmed cases  
                deaths
    Pivoted columns: 
        column name pattern: middle: _
        Pivot expression: 
            column name: count
            Expression: sum(daily_count)

5. Lookup Transformation
    Get data for country_code_2_digit (from lookup)
        Create dataset for lookup file: 
        Datasets -> New -> ADLS Gen2 -> Delimited Text
            Name: ds_country_lookup
            Linked Services: ls_adls_covidreportingdl
            File path: lookup/country_lookup.csv
            File header + Import schema 
            OK -> Preview data

        Source -> Output Stream name: CountryLookup
            Dataset: ds_country_lookup
            Projection: population - integer
            Data preview

    (+) on Pivot transformation -> Lookup
    Output stream name: LookupCountry
    Primary stream: PivotCounts
    Lookup stream: CountryLookup (will perform left-outer join)
    Match on: Any row 
    Lookup conditions: country == country
    Inpsect -> Data preview

    (+) on Lookup transformation -> Select
    Output stream name: SelectForSink

    PivotCounts@country -> country
    country_code_2_digit -> country_code_2_digit
    country_code_3_digit -> country_code_3_digit
    PivotCounts@population -> population
    confirmed_cases_count -> cases_count
    deaths_count -> deaths_count 
    reported_date -> reported_date
    source -> source

    Data preview

6. Sink Transformation
    (+) on Select transformation -> Sink
    Sink tab: 
        Output stream name: CasesAndDeathsSink
        Incoming stream: SelectForSink
        Sink type: Dataset
            (goal is to create a csv for sink file. But csv is not supported for inline. So we need to go with data set. )
            -> Create blob container under covidreportingdl: processed
        Dataset: New -> ADLS Gen2 -> Delimited Text
            Name: ds_processed_cases_and_deaths
            Linked Serivces: ls_adls_covidreportingdl
            File path: processed/ecdc/cases_deaths
            First row header: Yes
            Import schema: None
            Ok

    Settings tab: 
        Clear the folder: Yes

    Inspect -> Data preview 
    Validate -> Publish

7. Create ADF Pipeline
    Pipelines -> New -> Dataflow activity -> Use existing: df_transform_cases_deaths -> Ok
    Settings: 
        Compute size: small
    Name: pl_process_cases_and_deaths_data
    Debug
    Check progress
    Turn off Data flow debug (after completion)

    Open one of the files in generated in processed folder

    (Notes: to debug, you need to enable 'Data flow debug' or ADF will enable. Alternate is to create a trigger. ADF will create a cluster, execute the transformation and will destroy after pipleline is completed)

        Publish
        Add trigger -> Trigger now

    CasesAndDeathsSink: 
        Settings: File name option: Output to single file (not recommended)
        Optimize: ADF will change to single partition
        Output to single file: cases_and_deaths.csv
        Publish
        Add trigger -> Trigger now 
        One file will be generated in processed folder: verify data





